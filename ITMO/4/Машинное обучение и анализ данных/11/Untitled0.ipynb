{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ebFH66DweBv3","executionInfo":{"status":"ok","timestamp":1717066402462,"user_tz":-180,"elapsed":15,"user":{"displayName":"Chu Đoàn","userId":"17870500209712275967"}}},"outputs":[],"source":["# hyperparameters\n","epsilon = 0.1\n","gamma = 0.8\n","random_seed = 6\n","\n","\n","time_delay = 1 # time delay when rendering the game process after training (seconds)\n","lr_rate = 0.9 # alpha learning rate coefficient"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ce89FgiOeKcM","executionInfo":{"status":"ok","timestamp":1717066425864,"user_tz":-180,"elapsed":18657,"user":{"displayName":"Chu Đoàn","userId":"17870500209712275967"}},"outputId":"722cfc6e-12a2-4011-acff-9169204b9765"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","  0%|          | 0/10000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","100%|██████████| 10000/10000 [00:18<00:00, 538.38it/s]"]},{"output_type":"stream","name":"stdout","text":["1:  2202\n","2:  7473\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import gym\n","import numpy as np\n","import time\n","from IPython.display import clear_output\n","\n","\n","def generate_random_map(size, p, sd):\n","    \"\"\"Generates a random valid map (one that has a path from start to goal)\n","    :param size: size of each side of the grid\n","    :param p: probability that a tile is frozen\n","    \"\"\"\n","    valid = False\n","    np.random.seed(sd)\n","\n","    # DFS to check that it's a valid path.\n","    def is_valid(res):\n","        frontier, discovered = [], set()\n","        frontier.append((0,0))\n","        while frontier:\n","            r, c = frontier.pop()\n","            if not (r,c) in discovered:\n","                discovered.add((r,c))\n","                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n","                for x, y in directions:\n","                    r_new = r + x\n","                    c_new = c + y\n","                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n","                        continue\n","                    if res[r_new][c_new] == 'G':\n","                        return True\n","                    if (res[r_new][c_new] not in '#H'):\n","                        frontier.append((r_new, c_new))\n","        return False\n","\n","    while not valid:\n","        p = min(1, p)\n","        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n","        res[0][0] = 'S'\n","        res[-1][-1] = 'G'\n","        valid = is_valid(res)\n","    return [\"\".join(x) for x in res]\n","\n","# map generation\n","random_map = generate_random_map(size=6, p=0.8, sd = random_seed) # create our map\n","env = gym.make(\"FrozenLake-v1\", desc=random_map, is_slippery=False) # environment initialization\n","def choose_action(state):\n","    action=0\n","    if np.random.uniform(0, 1) < epsilon:\n","        action = np.random.randint(0,env.action_space.n)\n","    else:\n","        action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n","    return action\n","\n","def learn(state, state2, reward, action, done):\n","    Q[state, action] = Q[state, action] + lr_rate * (reward + gamma * max(Q[state2, :]) - Q[state, action])\n","from tqdm import tqdm\n","\n","# inititalization\n","np.random.seed(random_seed)\n","total_games = 10000\n","max_steps = 100\n","Q = np.zeros((env.observation_space.n, env.action_space.n))\n","totalWins = 0\n","fifthWinInRowId = -1\n","winInRowCounter = 0\n","inRowCountFlag = True\n","\n","# main cycle\n","for game in tqdm(range(total_games)):\n","    state = env.reset()\n","    t = 0\n","    while t < max_steps:\n","\n","        t += 1\n","\n","        action = choose_action(state)\n","\n","        state2, reward, done, info = env.step(action)\n","\n","        if t == max_steps:\n","            done = True\n","\n","        learn(state, state2, reward, action, done)\n","\n","        state = state2\n","\n","        if done:\n","            break\n","\n","    if reward == 1:\n","        totalWins += 1\n","        winInRowCounter += 1\n","    else:\n","      winInRowCounter = 0\n","\n","    if inRowCountFlag and (winInRowCounter == 5):\n","        fifthWinInRowId = game + 1\n","        inRowCountFlag = False\n","print(\"1: \", totalWins)\n","print(\"2: \", fifthWinInRowId)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMRz6zs1aUaXS2N0rYQum/t"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}